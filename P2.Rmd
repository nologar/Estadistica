---
title: "Trabajo Inferencia Estadistica"
output: html_document
date: "2024-11-13"
author:
  - Noé López garcía
  - Joan Pedro Bruixola
  - Carlos Ribes Garcia
  - Marc Velasco Mateu
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list=ls())

library(mclust)
library(ggplot2)
```


```{r datos, include=FALSE}
data<-quakes
```

1. Implementa una función en R para la función de densidad de probabilidad de la distribución de mezcla gaussiana. Grafica la función de densidad de probabilidad de $GM(2,1,5,1,0.3)$. Puedes usar la función incorporada `dnorm` para la función de densidad de probabilidad de la distribución gaussiana.

Implementamos la densidad de la siguiente forma:

```{r ej1.1, echo=TRUE}
densidadmg <- function(x, mu1, sd1, mu2, sd2, w) {
  densidad <- w * dnorm(x, mean = mu1, sd = sd1) + (1-w) * dnorm(x, mean = mu2, sd = sd2)
  return(densidad)
}
```

Así, podemos visualizar la densidad de la distribución $GM(2,1,5,1,0.3)$.

```{r ej1.2, echo=FALSE}
# Establecemos el rango de x
x <- seq(-1, 8, length.out = 500)

#Calculamos la densidad
densidad <- densidadmg(x, 2,1,5,1,0.3)

#Graficamos la densidad
plot(x, densidad, type = "l", col = "blue", lwd = 2,
     main = "Densidad de la Mezcla Gaussiana GM(2, 1, 5, 1, 0.3)",
     xlab = "x", ylab = "Densidad")
```

2.Inicialmente, solo observamos los datos de longitud y asumimos que las ubicaciones de longitud son i.i.d. que siguen un modelo de mezcla gaussiana. Estima los cinco parámetros de la mezcla gaussiana utilizando los 1000 valores observados de longitud. Puedes hacer esto numéricamente en R con la función `optim`. Grafica la mezcla gaussiana ajustada sobre el histograma de los datos de longitud.

Para encontrar un buen punto inicial para los parámetros, simplemente puedes observar el histograma de los datos y tratar de adivinar la ubicación de las medias $\mu_1$ y $\mu_2$. Una suposición inicial para $w$ puede ser la proporción del tamaño de los dos grupos de datos (o usar $w=0.5$ como suposición inicial). También puedes probar diferentes valores iniciales y reportar los resultados con la menor log-verosimilitud negativa.

Dado que hay muchos parámetros, la optimización puede llevar mucho tiempo y es probable que debas aumentar el número máximo de iteraciones del algoritmo; de lo contrario, este terminará antes de alcanzar un buen óptimo. Puedes hacerlo con `control = list(maxit = 10000)` en la función `optim`. Probablemente también habrá muchos *warnings*, principalmente porque los parámetros deben estar restringidos, especialmente $w$. Puedes ignorar los *warnings*.

Antes de empezar a buscar los parámetros, representemos el histograma de la variable longitud para encontrar un buen valor inicial para nuestra búsqueda de parámetros.

```{r ej2.1, echo=FALSE}
p <- ggplot(data, aes(x=long)) + 
  geom_histogram(color="black", fill="white", breaks=(floor(min(data$long))+0.5):(ceiling(max(data$long))-0.5)) +
  labs(title="Histograma de la variable longitud",x="Longitud", y = "Frecuencia")
p
```

Observando el histograma, podemos asignar como medias de las distribuciones separadas $167$ y $182$, pues son los picos más altos; desviaciones estándar $2$ y $2$, pues los datos no parecen estar muy esparcidos a partir de más de $2$ o $4$ unidades; y $w = 0.33$ pues la primera "montaña" tiene la mitad de altura que la segunda.

Utilizando estos datos como valores iniciales, obtenemos:

```{r ej2.2, echo=FALSE}
#Creamos la función de log-verosimilitud negativa
log_verosimilitud <- function(params, x) {
  mu1 <- params[1]
  sd1 <- params[2]
  mu2 <- params[3]
  sd2 <- params[4]
  w <- params[5]

  # Calcular la log-verosimilitud negativa
  densidades <- densidadmg(x, mu1, sd1, mu2, sd2, w)
  return(-sum(log(densidades)))
}

# Buscamos los valores optimizando
resultado <- optim(
  par = c(167,2,182,2,0.3),
  fn = log_verosimilitud,
  x = data$long,
  method = "L-BFGS-B",
  lower = c(-Inf, 0.01, -Inf, 0.01, 0.01),
  upper = c(Inf, Inf, Inf, Inf, 0.99)
)

cat("Estimación del parámetro mu1:",resultado$par[1],"\n")
cat("Estimación del parámetro sd1:",resultado$par[2],"\n")
cat("Estimación del parámetro mu2:",resultado$par[3],"\n")
cat("Estimación del parámetro sd2:",resultado$par[4],"\n")
cat("Estimación del parámetro w:",resultado$par[5],"\n")
```
Para comprobar si realmente estos son una buena estimación, dibujamos la densidad de una mezcla gaussiana de estos parámetros sobre el histograma anterior.

```{r ej2.3, echo=FALSE}
x <- seq(165,189,length.out = 240)
y <- densidadmg(x, resultado$par[1],resultado$par[2],resultado$par[3],resultado$par[4],resultado$par[5])

# Crear el histograma
hist(data$long, breaks = 30, probability = TRUE, col = "gray", xlab = "Longitud", ylab = "Densidad", main = "")

# Superponer la curva de densidad
title <- "Densidad de mezcla gausiana sobre histograma de la longitud"
lines(x, y, col = "blue", lwd = 2, lty = 1)
title(main = title)
```

Claramente, nuestra estimación era correcta, y los datos se ajustan perfectamente a esta distribución.

3.Considera ahora otro modelo en el que las ubicaciones de longitud son independientes y distribuidas de manera gaussiana $N(\mu,\sigma^2)$. Ajusta este modelo a los datos observados de longitud.

Como valores iniciales, escogemos la media y la desviación típica de nuestras observaciones, que en este caso son $179.462$ y $6.066461$.

De esta manera, obtenemos las estimaciones:

```{r ej3.1, echo=FALSE}
#Creamos la función de log-verosimilitud negativa
log_verosimilitud2 <- function(params, x) {
  mu <- params[1]
  sd <- params[2]

  # Calcular la log-verosimilitud negativa
  densidades <- dnorm(x, mean = mu, sd = sd)
  return(-sum(log(densidades)))
}

# Buscamos los valores optimizando
resultado2 <- optim(
  par = c(mean(data$long),sd(data$long)),
  fn = log_verosimilitud2,
  x = data$long,
  method = "L-BFGS-B",
  lower = c(-Inf, 0.01),
  upper = c(Inf, Inf)
)

cat("Estimación del parámetro mu:",resultado2$par[1],"\n")
cat("Estimación del parámetro sd:",resultado2$par[2],"\n")
```

Obviamente, las estimaciones coinciden con la media y la desviación típica de nuestros datos, pues estos estimadores son los estimadores de máxima verosimilitud para distribuciones normales.

Con estos parámetros estimados, podemos superponer nuestra densidad aproximada sobre el histograma, para estudiar visualmente si este sería un buen modelo.

```{r ej3.2, echo=FALSE}
x <- seq(165,189,length.out = 240)
y <- dnorm(x, resultado2$par[1],resultado2$par[2])

# Crear el histograma
hist(data$long, breaks = 30, probability = TRUE, col = "gray", xlab = "Longitud", ylab = "Densidad", main = "")

# Superponer la curva de densidad
title <- "Densidad de mezcla gausiana sobre histograma de la longitud"
lines(x, y, col = "blue", lwd = 2, lty = 1)
title(main = title)
```

Es obvio que no podemos asumir que nuestros datos siguen una distribución normal, pues el ajuste es pésimo.

4.Calcula los valores de AIC y BIC para el modelo gaussiano simple y el modelo de mezcla gaussiana para los datos de longitud. ¿Qué modelo debería seleccionarse?

```{r}
# Definimos las funciones para calcular el AIC y BIC a partir de las logverosimilitudes
AICfromL <- function(x, params, LogLike){
  return(  2*length(params) + 2*LogLike(params,x) ) #Sumamos en vez de restar porque las LogVerosimilitudes las hemos cambiado de signo
}
BICfromL <- function(x, params, LogLike){
  return( length(params)*log(length(x)) + 2*LogLike(params,x) ) # idem a AICfromL
}

#AIC y BIC del modelo gaussiano simple:

AICsimple <- AICfromL(data$long, params = resultado2$par, log_verosimilitud2)
BICsimple <- BICfromL(data$long, params = resultado2$par, log_verosimilitud2)

cat('\nResultados de los criterios AIC y BIC para el modelo simple: \nAIC:', AICsimple,'\nBIC:', BICsimple)

#AIC y BIC del modelo gaussiano ponderado:
AICponderado <- AICfromL(data$long, params = resultado$par, log_verosimilitud)
BICponderado <- BICfromL(data$long, params = resultado$par, log_verosimilitud)

cat('\n\nResultados de los criterios AIC y BIC para el modelo ponderado: \nAIC:', AICponderado,'\nBIC:', BICponderado)


```
Con lo que se tiene que ambos criterios se decantan por el modelo ponderado ante el modelo simple.

5.Repite el procedimiento de ajuste anterior para los datos de latitud y profundidad, y realiza la selección de modelos como de costumbre utilizando AIC y BIC. ¿Qué modelo debería seleccionarse?

En primer lugar hacemos el estudio de latitud:
```{r}
resultado_lat_simple <- optim(
  par = c(mean(data$lat), sd(data$lat)),
  fn = log_verosimilitud2,
  x = data$lat,
  method = "L-BFGS-B",
  lower = c(-Inf, 0.01, -Inf, 0.01, 0.01),
  upper = c(Inf, Inf, Inf, Inf, 0.99)
)

resultado_lat_pond <- optim(
  par = c(15,2,25,2,0.3),
  fn = log_verosimilitud,
  x = data$lat,
  method = "L-BFGS-B",
  lower = c(-Inf, 0.01, -Inf, 0.01, 0.01),
  upper = c(Inf, Inf, Inf, Inf, 0.99)
)

AIC_lat_simple <- AICfromL(data$lat, params = resultado_lat_simple$par, log_verosimilitud2)
BIC_lat_simple <- BICfromL(data$lat, params = resultado_lat_simple$par, log_verosimilitud2)

cat('\nResultados de los criterios AIC y BIC para el modelo simple aplicado a latitud son: \nAIC:', AIC_lat_simple,'\nBIC:', BIC_lat_simple)

AIC_lat_pond <- AICfromL(data$lat, params = resultado_lat_pond$par, log_verosimilitud)
BIC_lat_pond <- BICfromL(data$lat, params = resultado_lat_pond$par, log_verosimilitud)

cat('\nResultados de los criterios AIC y BIC para el modelo ponderado aplicado a latitud son: \nAIC:', AIC_lat_pond,'\nBIC:', BIC_lat_pond)

```
Y a continuación de la profundidad:
```{r}
resultado_prof_simple <- optim(
  par = c(mean(data$depth), sd(data$depth)),
  fn = log_verosimilitud2,
  x = data$depth,
  method = "L-BFGS-B",
  lower = c(-Inf, 0.01, -Inf, 0.01, 0.01),
  upper = c(Inf, Inf, Inf, Inf, 0.99)
)

hist(data$depth)

resultado_prof_pond <- optim(
  par = c(100,2,600,2,0.6), #Inicializamos viendo en el histograma dónde parecen estar los dos centros de las gaussianas
  fn = log_verosimilitud,
  x = data$depth,
  method = "L-BFGS-B",
  lower = c(-Inf, 0.01, -Inf, 0.01, 0.01),
  upper = c(Inf, Inf, Inf, Inf, 0.99)
)

AIC_prof_simple <- AICfromL(data$lat, params = resultado_lat_simple$par, log_verosimilitud2)
BIC_prof_simple <- BICfromL(data$lat, params = resultado_lat_simple$par, log_verosimilitud2)

cat('\nResultados de los criterios AIC y BIC para el modelo simple aplicado a latitud son: \nAIC:', AIC_prof_simple,'\nBIC:', BIC_prof_simple)

AIC_prof_pond <- AICfromL(data$lat, params = resultado_prof_pond$par, log_verosimilitud)
BIC_prof_pond <- BICfromL(data$lat, params = resultado_prof_pond$par, log_verosimilitud)

cat('\nResultados de los criterios AIC y BIC para el modelo ponderado aplicado a latitud son: \nAIC:', AIC_prof_pond,'\nBIC:', BIC_prof_pond)

```
En este caso vemos que el modelo simple nos da mejores resultados tanto para el criterio AIC como para el BIC.


6.Considera los dos grupos de eventos y calcula el valor medio e intervalos de confianza (95 %) para la media de las longitudes, latitudes y profundidades en los dos grupos (azul y rojo en la Figura 1). ¿Qué conclusiones podemos sacar de los intervalos de confianza?
```{r}

```


7.Consideramos ahora la variable número de estaciones sísmicas que detectaron el evento (stations). ¿Podemos afirmar que el número medio de estaciones que detectaron los eventos es significativamente distinto en los dos grupos de terremotos (a nivel 𝛼=0.01α=0.01 y a nivel 𝛼=0.1α=0.1)? (Puedes utilizar bootstrap y/o asumir poblaciones normales).

```{r}

```


