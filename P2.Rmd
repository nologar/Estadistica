---
title: "Trabajo Inferencia Estadistica"
output: html_document
date: "2024-11-13"
author:
  - Noé López garcía
  - Joan Pedro Bruixola
  - Carlos Ribes Garcia
  - Marc Velasco Mateu
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

rm(list=ls())

library(mclust)
library(ggplot2)
library(dplyr)
library(tidyr)
```


```{r datos, include=FALSE}
data<-quakes
```

## Ejercicio 1
**Implementa una función en R para la función de densidad de probabilidad de la distribución de mezcla gaussiana. Grafica la función de densidad de probabilidad de $GM(2,1,5,1,0.3)$. Puedes usar la función incorporada `dnorm` para la función de densidad de probabilidad de la distribución gaussiana.**

Implementamos la densidad de la siguiente forma:

```{r ej1.1, echo=TRUE}
densidadmg <- function(x, mu1, sd1, mu2, sd2, w) {
  densidad <- w * dnorm(x, mean = mu1, sd = sd1) + (1-w) * dnorm(x, mean = mu2, sd = sd2)
  return(densidad)
}
```

Así, podemos visualizar la densidad de la distribución $GM(2,1,5,1,0.3)$.

```{r ej1.2, echo=FALSE}
# Establecemos el rango de x
x <- seq(-1, 8, length.out = 500)

#Calculamos la densidad
densidad <- densidadmg(x, 2,1,5,1,0.3)

#Graficamos la densidad
plot(x, densidad, type = "l", col = "blue", lwd = 2,
     main = "Densidad de la Mezcla Gaussiana GM(2, 1, 5, 1, 0.3)",
     xlab = "x", ylab = "Densidad")
```

## Ejercicio 2
**Inicialmente, solo observamos los datos de longitud y asumimos que las ubicaciones de longitud son i.i.d. que siguen un modelo de mezcla gaussiana. Estima los cinco parámetros de la mezcla gaussiana utilizando los 1000 valores observados de longitud. Puedes hacer esto numéricamente en R con la función `optim`. Grafica la mezcla gaussiana ajustada sobre el histograma de los datos de longitud.**

**Para encontrar un buen punto inicial para los parámetros, simplemente puedes observar el histograma de los datos y tratar de adivinar la ubicación de las medias $\mu_1$ y $\mu_2$. Una suposición inicial para $w$ puede ser la proporción del tamaño de los dos grupos de datos (o usar $w=0.5$ como suposición inicial). También puedes probar diferentes valores iniciales y reportar los resultados con la menor log-verosimilitud negativa.**

**Dado que hay muchos parámetros, la optimización puede llevar mucho tiempo y es probable que debas aumentar el número máximo de iteraciones del algoritmo; de lo contrario, este terminará antes de alcanzar un buen óptimo. Puedes hacerlo con `control = list(maxit = 10000)` en la función `optim`. Probablemente también habrá muchos *warnings*, principalmente porque los parámetros deben estar restringidos, especialmente $w$. Puedes ignorar los *warnings*.**

Antes de empezar a buscar los parámetros, representemos el histograma de la variable longitud para encontrar un buen valor inicial para nuestra búsqueda de parámetros.

```{r ej2.1, echo=FALSE}
p <- ggplot(data, aes(x=long)) + 
  geom_histogram(color="black", fill="white", breaks=(floor(min(data$long))+0.5):(ceiling(max(data$long))-0.5)) +
  labs(title="Histograma de la variable longitud",x="Longitud", y = "Frecuencia")
p
```

Observando el histograma, podemos asignar como medias de las distribuciones separadas $167$ y $182$, pues son los picos más altos; desviaciones estándar $2$ y $2$, pues los datos no parecen estar muy esparcidos a partir de más de $2$ o $4$ unidades; y $w = 0.33$ pues la primera "montaña" tiene la mitad de altura que la segunda.

Utilizando estos datos como valores iniciales, obtenemos:

```{r ej2.2, echo=FALSE}
#Creamos la función de log-verosimilitud negativa
log_verosimilitud <- function(params, x) {
  mu1 <- params[1]
  sd1 <- params[2]
  mu2 <- params[3]
  sd2 <- params[4]
  w <- params[5]

  # Calcular la log-verosimilitud negativa
  densidades <- densidadmg(x, mu1, sd1, mu2, sd2, w)
  return(-sum(log(densidades)))
}

# Buscamos los valores optimizando
resultado <- optim(
  par = c(167,2,182,2,0.3),
  fn = log_verosimilitud,
  x = data$long,
  method = "L-BFGS-B",
  lower = c(-Inf, 0.01, -Inf, 0.01, 0.01),
  upper = c(Inf, Inf, Inf, Inf, 0.99)
)

cat("Estimación del parámetro mu1:",resultado$par[1],"\n")
cat("Estimación del parámetro sd1:",resultado$par[2],"\n")
cat("Estimación del parámetro mu2:",resultado$par[3],"\n")
cat("Estimación del parámetro sd2:",resultado$par[4],"\n")
cat("Estimación del parámetro w:",resultado$par[5],"\n")
```
Para comprobar si realmente estos son una buena estimación, dibujamos la densidad de una mezcla gaussiana de estos parámetros sobre el histograma anterior.

```{r ej2.3, echo=FALSE}
x <- seq(165,189,length.out = 240)
y <- densidadmg(x, resultado$par[1],resultado$par[2],resultado$par[3],resultado$par[4],resultado$par[5])

# Crear el histograma
hist(data$long, breaks = 30, probability = TRUE, col = "gray", xlab = "Longitud", ylab = "Densidad", main = "")

# Superponer la curva de densidad
title <- "Densidad de mezcla gausiana sobre histograma de la longitud"
lines(x, y, col = "blue", lwd = 2, lty = 1)
title(main = title)
```

Claramente, nuestra estimación era correcta, y los datos se ajustan perfectamente a esta distribución.

## Ejercicio 3
**Considera ahora otro modelo en el que las ubicaciones de longitud son independientes y distribuidas de manera gaussiana $N(\mu,\sigma^2)$. Ajusta este modelo a los datos observados de longitud.**

Como valores iniciales, escogemos la media y la desviación típica de nuestras observaciones, que en este caso son $179.462$ y $6.066461$.

De esta manera, obtenemos las estimaciones:

```{r ej3.1, echo=FALSE}
#Creamos la función de log-verosimilitud negativa
log_verosimilitud2 <- function(params, x) {
  mu <- params[1]
  sd <- params[2]

  # Calcular la log-verosimilitud negativa
  densidades <- dnorm(x, mean = mu, sd = sd)
  return(-sum(log(densidades)))
}

# Buscamos los valores optimizando
resultado2 <- optim(
  par = c(mean(data$long),sd(data$long)),
  fn = log_verosimilitud2,
  x = data$long,
  method = "L-BFGS-B",
  lower = c(-Inf, 0.01),
  upper = c(Inf, Inf)
)

cat("Estimación del parámetro mu:",resultado2$par[1],"\n")
cat("Estimación del parámetro sd:",resultado2$par[2],"\n")
```

Obviamente, las estimaciones coinciden con la media y la desviación típica de nuestros datos, pues estos estimadores son los estimadores de máxima verosimilitud para distribuciones normales.

Con estos parámetros estimados, podemos superponer nuestra densidad aproximada sobre el histograma, para estudiar visualmente si este sería un buen modelo.

```{r ej3.2, echo=FALSE}
x <- seq(165,189,length.out = 240)
y <- dnorm(x, resultado2$par[1],resultado2$par[2])

# Crear el histograma
hist(data$long, breaks = 30, probability = TRUE, col = "gray", xlab = "Longitud", ylab = "Densidad", main = "")

# Superponer la curva de densidad
title <- "Densidad de mezcla gausiana sobre histograma de la longitud"
lines(x, y, col = "blue", lwd = 2, lty = 1)
title(main = title)
```

Es obvio que no podemos asumir que nuestros datos siguen una distribución normal, pues el ajuste es pésimo.

## Ejercicio 4
**Calcula los valores de AIC y BIC para el modelo gaussiano simple y el modelo de mezcla gaussiana para los datos de longitud. ¿Qué modelo debería seleccionarse?**

```{r ej4.1, echo=FALSE}
# Definimos las funciones para calcular el AIC y BIC a partir de las logverosimilitudes
AICfromL <- function(x, params, LogLike){
  return(  2*length(params) + 2*LogLike(params,x) ) #Sumamos en vez de restar porque las LogVerosimilitudes las hemos cambiado de signo
}
BICfromL <- function(x, params, LogLike){
  return( length(params)*log(length(x)) + 2*LogLike(params,x) ) # idem a AICfromL
}

#AIC y BIC del modelo gaussiano simple:
AICsimple <- AICfromL(data$long, params = resultado2$par, log_verosimilitud2)
BICsimple <- BICfromL(data$long, params = resultado2$par, log_verosimilitud2)

cat('Resultados de los criterios AIC y BIC para el modelo simple: \nAIC:', AICsimple,'\nBIC:', BICsimple)

#AIC y BIC del modelo gaussiano ponderado:
AICponderado <- AICfromL(data$long, params = resultado$par, log_verosimilitud)
BICponderado <- BICfromL(data$long, params = resultado$par, log_verosimilitud)

cat('Resultados de los criterios AIC y BIC para el modelo ponderado: \nAIC:', AICponderado,'\nBIC:', BICponderado)
```
Como vemos, ambos criterios se decantan por el modelo ponderado ante el modelo simple.

## Ejercicio 5
**Repite el procedimiento de ajuste anterior para los datos de latitud y profundidad, y realiza la selección de modelos como de costumbre utilizando AIC y BIC. ¿Qué modelo debería seleccionarse?**

En primer lugar hacemos el estudio de latitud:

```{r ej5.1, echo = FALSE, warning=FALSE}
resultado_lat_simple <- optim(
  par = c(mean(data$lat), sd(data$lat)),
  fn = log_verosimilitud2,
  x = data$lat,
  method = "L-BFGS-B",
  lower = c(-Inf, 0.01, -Inf, 0.01, 0.01),
  upper = c(Inf, Inf, Inf, Inf, 0.99)
)

resultado_lat_pond <- optim(
  par = c(15,2,25,2,0.3),
  fn = log_verosimilitud,
  x = data$lat,
  method = "L-BFGS-B",
  lower = c(-Inf, 0.01, -Inf, 0.01, 0.01),
  upper = c(Inf, Inf, Inf, Inf, 0.99)
)

AIC_lat_simple <- AICfromL(data$lat, params = resultado_lat_simple$par, log_verosimilitud2)
BIC_lat_simple <- BICfromL(data$lat, params = resultado_lat_simple$par, log_verosimilitud2)

cat('Resultados de los criterios AIC y BIC para el modelo simple aplicado a latitud son: \nAIC:', AIC_lat_simple,'\nBIC:', BIC_lat_simple)

AIC_lat_pond <- AICfromL(data$lat, params = resultado_lat_pond$par, log_verosimilitud)
BIC_lat_pond <- BICfromL(data$lat, params = resultado_lat_pond$par, log_verosimilitud)

cat('Resultados de los criterios AIC y BIC para el modelo ponderado aplicado a latitud son: \nAIC:', AIC_lat_pond,'\nBIC:', BIC_lat_pond)

densidadobtenidalat  <- densidadmg( data$lat, resultado_lat_pond$par[1], resultado_lat_pond$par[2], resultado_lat_pond$par[3], resultado_lat_pond$par[4], resultado_lat_pond$par[5] )

ggplot(data = data)+
  geom_histogram( aes(x = lat, y = ..density..), binwidth=1 )+
  geom_line( aes( x = lat, y = densidadobtenidalat), linewidth = 1, col = "red")+
  geom_line( aes( x = lat,y = dnorm(lat, resultado_lat_simple$par[1],resultado_lat_simple$par[2])), linewidth = 1, col = "blue")
```

Vemos que los modelos són prácticamente idénticos, lo que se debe a que el modelo de mezcla ha considerado prácticamente solo una gaussiana:

```{r ej5.2, echo=FALSE}
cat("Parámetro peso estimado:",resultado_lat_pond$par[5])
```

Es por este motivo que el modelo simple, que tiene menos parámetros, salga beneficiado del criterio AIC/BIC.

Hacemos ahora un estudio de la distribución de la profundidad:

```{r ej5.3,echo=FALSE, warning = FALSE}
resultado_prof_simple <- optim(
  par = c(mean(data$depth), sd(data$depth)),
  fn = log_verosimilitud2,
  x = data$depth,
  method = "L-BFGS-B",
  lower = c(-Inf, 0.01, -Inf, 0.01, 0.01),
  upper = c(Inf, Inf, Inf, Inf, 0.99)
)

resultado_prof_pond <- optim(
  par = c(75,60,600,150,0.7), #Inicializamos viendo en el histograma dónde parecen estar los dos centros de las gaussianas 170, 600
  fn = log_verosimilitud,
  x = data$depth,
  method = "L-BFGS-B",
  lower = c(0, 0.01, -Inf, 0.01, 0.01),
  upper = c(Inf, Inf, Inf, Inf, 0.99)
)

AIC_prof_simple <- AICfromL(data$lat, params = resultado_lat_simple$par, log_verosimilitud2)
BIC_prof_simple <- BICfromL(data$lat, params = resultado_lat_simple$par, log_verosimilitud2)

cat('Resultados de los criterios AIC y BIC para el modelo simple aplicado a profundidad son: \nAIC:', AIC_prof_simple,'\nBIC:', BIC_prof_simple)

AIC_prof_pond <- AICfromL(data$lat, params = resultado_prof_pond$par, log_verosimilitud)
BIC_prof_pond <- BICfromL(data$lat, params = resultado_prof_pond$par, log_verosimilitud)

cat('Resultados de los criterios AIC y BIC para el modelo ponderado aplicado a profundidad son: \nAIC:', AIC_prof_pond,'\nBIC:', BIC_prof_pond)

Xaxis <- seq(-200,800, by = 1)
densidadobtenidaX <- densidadmg( Xaxis, resultado_prof_pond$par[1], resultado_prof_pond$par[2], resultado_prof_pond$par[3], resultado_prof_pond$par[4], resultado_prof_pond$par[5] )
densidadobtenidadepth  <- densidadmg( data$depth, resultado_prof_pond$par[1], resultado_prof_pond$par[2], resultado_prof_pond$par[3], resultado_prof_pond$par[4], resultado_prof_pond$par[5] )

ggplot(data = data)+
  geom_histogram( aes(x = depth, y = ..density..), binwidth = 10 )+
  geom_line( aes( x = depth, y = densidadobtenidadepth), linewidth = 1, col = "red")+
  geom_line( aes( x = depth,y = dnorm(depth, resultado_prof_simple$par[1],resultado_prof_simple$par[2])), linewidth = 1, col = "blue")
```

En este caso vemos que el modelo simple nos da mejores resultados tanto para el criterio AIC como para el BIC. Y esto puede desconcertar un poco en vista del aspecto de la gráfica anterior, pero tenemos que tener en cuenta que las funciones de densidad gaussianas tienen soporte infinito, y por tanto, si visualzamos un poco más del eje X:

```{r ej5.4, echo=FALSE}
ggplot()+
  geom_histogram( data = data, aes(x = depth, y = ..density..) , binwidth = 10)+
  geom_line( aes( x = Xaxis, y = densidadobtenidaX), linewidth = 1, col = "red")+
  geom_line( aes( x = Xaxis,y = dnorm(Xaxis, resultado_prof_simple$par[1],resultado_prof_simple$par[2])), linewidth = 1, col = "blue")
```

Nos damos cuenta de que ambos modelos fallan en estos puntos fuera del rango de valores de 'depth', con lo que no es extraño que se priorice un modelo con 3 parámetros menos.

## Ejercicio 6
**Considera los dos grupos de eventos y calcula el valor medio e intervalos de confianza (95%) para la media de las longitudes, latitudes y profundidades en los dos grupos (azul y rojo en la Figura 1). ¿Qué conclusiones podemos sacar de los intervalos de confianza?**

```{r ej6.1, message=FALSE, warning=FALSE,echo=FALSE}
data$class <- ifelse(data$long > 175, "Tonga trench", "Plate junction")

# Separamos en un dataset por evento

data_Tonga<-data %>%
  filter(class=="Tonga trench")

data_Plate<- data %>%
  filter(class=="Plate junction")

# Función para calcular intervalos de confianza (formato aseado)
intervalo_confianza <- function(x, nivel = 0.95) {
  n <- length(x)
  mean_x <- mean(x)
  se <- sd(x) / sqrt(n)  # Error estándar
  error <- qt((1 + nivel) / 2, df = n - 1) * se
  c(Media = mean_x, LI = mean_x - error, LS = mean_x + error)
}

# Procesar datos para Tonga Trench
resultados_tonga <- data_Tonga %>%
  summarise(
    Longitud = intervalo_confianza(long),
    Latitud = intervalo_confianza(lat),
    Profundidad = intervalo_confianza(depth)
  )

# Trasponer y renombrar columnas para Tonga
resultados_tonga <- as.data.frame(t(resultados_tonga))  # Transponer
colnames(resultados_tonga) <- c("Media", "Valor inferior", "Valor superior")  # Renombrar columnas

# Procesar datos para Plate Junction
resultados_plate <- data_Plate %>%
  summarise(
    Longitud = intervalo_confianza(long),
    Latitud = intervalo_confianza(lat),
    Profundidad = intervalo_confianza(depth)
  )

# Trasponer y renombrar columnas para Plate
resultados_plate <- as.data.frame(t(resultados_plate))  # Transponer
colnames(resultados_plate) <- c("Media", "Valor inferior", "Valor superior")  # Renombrar columnas
```

Empleando la distinción del archivo quakes_plot se ha separado el dataframe en los datos corresponientes a la Tonga y a unión de las placas. Primeramente, se muestran los resultados obtenidos para los datos correspondientes a la Tonga.

```{r ej6.2 , message=FALSE, warning=FALSE,echo=FALSE}
# Mostrar resultados
# Resultados de Tonga
print(resultados_tonga)
```

Ahora se muestran los resultados para la unión de las placas.

```{r ej6.3, message=FALSE, warning=FALSE,echo=FALSE}
#Resultados para Plate junction
print(resultados_plate)
```

Finalmente, como vemos los intervalos de confianza nos permiten estimar el rango en el que se encuentra la media verdadera de las variables analizadas (longitud, latitud y profundidad) con un nivel de confianza del 95 %. En este caso, se observa que los intervalos de confianza de cada variable no se solapan entre los dos grupos (Tonga trench y Plate junction). Esto confirma que existen diferencias estadísticamente significativas entre ambos conjuntos de eventos.

En particular, las diferencias en las medias y los intervalos reflejan características claramente distintas:

* En longitud, los eventos del grupo Tonga trench se encuentran en regiones más al este, mientras que Plate junction está más al oeste. Esto es coherente con lo que se ve claramente en el mapa que se nos proporciona en la práctica.

* En latitud, los eventos de Tonga trench están ubicados en su mayoría más al sur, en comparación con Plate junction.

* En profundidad, los eventos de Tonga trench se producen a mayores profundidades, lo que es consistente pues se trata de una fosa , mientras que los eventos de Plate junction ocurren en zonas más superficiales.

Estos resultados muestran que nos encontramos ante dos fenómenos claramente distintos.

## Ejercicio 7
**Consideramos ahora la variable número de estaciones sísmicas que detectaron el evento (stations). ¿Podemos afirmar que el número medio de estaciones que detectaron los eventos es significativamente distinto en los dos grupos de terremotos (a nivel 𝛼=0.01 y a nivel α=0.1)? (Puedes utilizar bootstrap y/o asumir poblaciones normales).**

Para responder a la pregunta que se plantea realizaremos un contraste de hipótesis sobre las medias de los grupos:
$H_{0}: \mu_{1} = \mu_{2}$, $H_{1}: \mu_{1} \neq \mu_{2}$.

```{r ej7.1, echo=FALSE}
# Dividir los datos en dos grupos según la magnitud
grupo1 <- data$stations[data$class == "Tonga trench"]
grupo2 <- data$stations[data$class == "Plate junction"]
```

El contraste lo podríamos resolver mediante un test t-student bilateral, pero primero tenemos que estudiar si las poblaciones son aproximadamente normales. Primero, haremos un histograma y veremos si las distribuciones parecen normales:

```{r ej7.2, echo=FALSE}
par(mfrow = c(1, 2)) 

# Histograma para el grupo "Tonga trench"
hist(grupo1, 
     breaks = 20, 
     col = "lightblue", 
     main = "Tonga trench", 
     xlab = "Stations", 
     probability = TRUE)
lines(density(grupo1), col = "darkblue", lwd = 2)

# Histograma para el grupo "Plate junction"
hist(grupo2, 
     breaks = 20, 
     col = "lightgreen", 
     main = "Plate junction", 
     xlab = "Stations", 
     probability = TRUE)
lines(density(grupo2), col = "darkgreen", lwd = 2)
```

Observamos que los histogramas nos muestran una distribución de los datos que no parece normal. Para confirmarlo, hacemos un test shapiro-wilk:

```{r ej7.3, echo=FALSE}
shapiro.test(grupo1) # para el grupo Tonga trench
shapiro.test(grupo2) # para el grupo Plate junction
```

Por tanto, podemos confirmar que los grupos no siguen una distribución normal. Esto implica que no podemos asumir normalidad, y por tanto no podemos aplicar un test t-student. Por eso, utilizaremos una técnica bootstrap. Lo que haremos es hacer intervalos de confianzaa niveles 99% y 90% para las diferencias de las medias de muestras generadas con bootstrap y veremos si el 0 está. Es decir, estamos resolviendo el contraste: $H_{0}: \mu_{1} - \mu_{2} = 0$, $H_{1}: \mu_{1} - \mu_{2} \neq 0$.

```{r ej7.4, echo=FALSE}
set.seed(123) # Para reproducibilidad

# Crear una función para calcular la diferencia de medias
diff_means <- function(data1, data2) {
  mean(data1) - mean(data2)
}

# Bootstrap
n_boot <- 10000
boot_diff <- replicate(n_boot, {
  sample1 <- sample(grupo1, length(grupo1), replace = TRUE)
  sample2 <- sample(grupo2, length(grupo2), replace = TRUE)
  diff_means(sample1, sample2)
})

# Intervalo de confianza al 99% y al 90%
ci_99 <- quantile(boot_diff, probs = c(0.005, 0.995))
ci_90 <- quantile(boot_diff, probs = c(0.05, 0.95))

print(ci_99)
print(ci_90)

# Verificar si el intervalo incluye 0
if (ci_99[1] > 0 || ci_99[2] < 0) {
  cat("Rechazamos H0 al nivel de significancia 0.01\n")
} else {
  cat("No podemos rechazar H0 al nivel de significancia 0.01\n")
}

if (ci_90[1] > 0 || ci_90[2] < 0) {
  cat("Rechazamos H0 al nivel de significancia 0.1\n")
} else {
  cat("No podemos rechazar H0 al nivel de significancia 0.1\n")
}
```

Es decir, con un nivel de significancia de 0.01 no tenemos evidencia suficiente para decir que las medias de los grupos no son iguales. En cambio, para un nivel de 0.1 si que tenemos evidencia estadística suficiente para afirmar que las medias de los grupos son distintas.